{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 1: Behavior Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(6,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Strategy:\n",
    "1. Collect expert rollouts\n",
    "2. Create neural network that parameterizes the policy pi_theta (a_t | o_t)\n",
    "3. Train network on rollout data (o_t = x_train, a_t = y_train)\n",
    "4. Visualize performance of network\n",
    "    - render the frames\n",
    "    - measure similarity between distributions (cross entropy / KL div)\n",
    "\n",
    "Notes:\n",
    "- All expert envs have continuous actions\n",
    "- Actions are continuous so we'll need to output logit scores\n",
    "- We'll use the MSE loss \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import gym\n",
    "env = gym.make(\"Walker2d-v2\")\n",
    "print(env.action_space)\n",
    "import torch\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral Cloning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Behavioral Cloning Neural Network\n",
      "Action space: Box(8,)\n",
      "Observation space: Box(111,)\n",
      "Environment: Ant-v2\n",
      "BCPolicy(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=111, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=8, bias=True)\n",
      "  )\n",
      ")\n",
      "======================================================================\n",
      "======================================================================\n",
      "LOADED EXPERT DATA\n",
      "Data points 196845\n",
      "X_train torch.Size([177161, 111])\n",
      "y_train torch.Size([177161, 8])\n",
      "X_val torch.Size([19684, 111])\n",
      "X_val torch.Size([19684, 8])\n",
      "======================================================================\n",
      "Epoch 0 Training loss 0.10178931\n",
      "Epoch 1 Training loss 0.092089154\n",
      "Epoch 2 Training loss 0.08420419\n",
      "Epoch 3 Training loss 0.077312835\n",
      "Epoch 4 Training loss 0.071023256\n",
      "Epoch 5 Training loss 0.06513419\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-762eda6e000b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deeprl/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/deeprl/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# follow this\n",
    "# https://github.com/KuNyaa/berkeleydeeprlcourse-homework-pytorch-solution/blob/master/hw1/train.py\n",
    "\n",
    "def print_summary(title,policy,env_name):\n",
    "    env = gym.make(env_name)\n",
    "    print('='*70)\n",
    "    print(title)\n",
    "    print('Action space:',env.action_space)\n",
    "    print('Observation space:',env.observation_space)\n",
    "    print('Environment:',env_name)\n",
    "    print(policy)\n",
    "    print('='*70)\n",
    "\n",
    "def load_data(env_name,verbose=False):\n",
    "    with open('./expert_data/'+env_name+'.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    X_train = data['observations']\n",
    "    y_train = data['actions'].reshape(-1,act_dim)\n",
    "    N = X_train.shape[0]\n",
    "    val_ratio = 0.1\n",
    "    rand_ids = np.random.choice(range(N),N,replace=False)\n",
    "    M = int(val_ratio * N)\n",
    "    \n",
    "    val_ids = rand_ids[:M]\n",
    "    #test_ids = rand_ids[M:2*M]\n",
    "    train_ids = rand_ids[M:]\n",
    "    \n",
    "    #X_train -= X_train.mean(0).reshape(-1,1)\n",
    "    #X_train /= X_train.std(0).reshape(-1,1)\n",
    "    \n",
    "    X_val = torch.tensor(X_train[val_ids]).float()\n",
    "    y_val = torch.tensor(y_train[val_ids]).float()\n",
    "\n",
    "    X_train = torch.tensor(X_train[train_ids]).float()\n",
    "    y_train = torch.tensor(y_train[train_ids]).float()\n",
    "    \n",
    "    if verbose:\n",
    "        print('='*70)\n",
    "        print('LOADED EXPERT DATA')\n",
    "        print('Data points',N)\n",
    "        print('X_train',X_train.shape)\n",
    "        print('y_train',y_train.shape)\n",
    "        print('X_val',X_val.shape)\n",
    "        print('X_val',y_val.shape)\n",
    "        #print('X_test',X_test.shape)\n",
    "        #print('y_test',y_test.shape)\n",
    "        print('='*70)\n",
    "    \n",
    "    return X_train,y_train,X_val,y_val\n",
    "\n",
    "class BCPolicy(nn.Module):\n",
    "    \n",
    "    def __init__(self,obs_dim,h_dim,act_dim):\n",
    "        super(BCPolicy,self).__init__()\n",
    "        \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(obs_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h_dim, act_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.layers(X)\n",
    "\n",
    "def MSE_loss(y_pred,y):\n",
    "    return torch.mean((y_pred-y)**2) \n",
    "\n",
    "\n",
    "title = 'Behavioral Cloning Neural Network'\n",
    "env_name = \"Ant-v2\"\n",
    "env = gym.make(env_name)\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# hyperparameters\n",
    "h_dim = 128\n",
    "learning_rate = 1e-3\n",
    "epochs = 20\n",
    "batch_size = 2000\n",
    "episodes = 10\n",
    "max_steps = env._max_episode_steps\n",
    "\n",
    "policy = BCPolicy(obs_dim,h_dim,act_dim)\n",
    "\n",
    "optimizer = optim.Adam(policy.parameters(),lr=learning_rate)\n",
    "\n",
    "print_summary(title,policy,env_name)\n",
    "\n",
    "X_train,y_train,X_val,y_val = load_data(env_name,True)\n",
    "\n",
    "y_pred_val = policy(X_val)\n",
    "score = MSE_loss(y_pred_val,y_val)\n",
    "\n",
    "#print('Validation set MSE')\n",
    "#print(score)\n",
    "#print('='*70)\n",
    "\n",
    "N = X_train.shape[0]\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "# train and measure validation loss\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    # sample minibatch\n",
    "    rand_ids = np.random.choice(range(N),batch_size)\n",
    "    X_batch = X_train#[rand_ids]\n",
    "    y_batch = y_train#[rand_ids]\n",
    "    y_pred = policy(X_batch)\n",
    "\n",
    "    y_val_pred = policy(X_val)\n",
    "    loss = mse_loss(y_pred, y_batch)\n",
    "    #loss_val = mse_loss(y_val_pred, y_val).detach().numpy()\n",
    "    \n",
    "    print('Epoch',epoch,'Training loss',loss.detach().numpy())#,'Validation loss',loss_val)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "# test the policy\n",
    "\n",
    "for episode in range(episodes):\n",
    "    returns = 0.0\n",
    "    s = env.reset()\n",
    "    with torch.no_grad():\n",
    "        for step in itertools.count():\n",
    "            s = torch.tensor(s).float()\n",
    "            a = policy(s).detach().numpy()\n",
    "            s,r,d,_ = env.step(a)\n",
    "            returns+=r\n",
    "            if d:\n",
    "                break\n",
    "    print(returns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, tensorflow as tf, tf_util, numpy as np\n",
    "\n",
    "def load_policy(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.loads(f.read())\n",
    "\n",
    "    # assert len(data.keys()) == 2\n",
    "    nonlin_type = data['nonlin_type']\n",
    "    policy_type = [k for k in data.keys() if k != 'nonlin_type'][0]\n",
    "\n",
    "    assert policy_type == 'GaussianPolicy', 'Policy type {} not supported'.format(policy_type)\n",
    "    policy_params = data[policy_type]\n",
    "\n",
    "    assert set(policy_params.keys()) == {'logstdevs_1_Da', 'hidden', 'obsnorm', 'out'}\n",
    "\n",
    "    # Keep track of input and output dims (i.e. observation and action dims) for the user\n",
    "\n",
    "    def build_policy(obs_bo):\n",
    "        def read_layer(l):\n",
    "            assert list(l.keys()) == ['AffineLayer']\n",
    "            assert sorted(l['AffineLayer'].keys()) == ['W', 'b']\n",
    "            return l['AffineLayer']['W'].astype(np.float32), l['AffineLayer']['b'].astype(np.float32)\n",
    "\n",
    "        def apply_nonlin(x):\n",
    "            if nonlin_type == 'lrelu':\n",
    "                return tf_util.lrelu(x, leak=.01) # openai/imitation nn.py:233\n",
    "            elif nonlin_type == 'tanh':\n",
    "                return tf.tanh(x)\n",
    "            else:\n",
    "                raise NotImplementedError(nonlin_type)\n",
    "\n",
    "        # Build the policy. First, observation normalization.\n",
    "        assert list(policy_params['obsnorm'].keys()) == ['Standardizer']\n",
    "        obsnorm_mean = policy_params['obsnorm']['Standardizer']['mean_1_D']\n",
    "        obsnorm_meansq = policy_params['obsnorm']['Standardizer']['meansq_1_D']\n",
    "        obsnorm_stdev = np.sqrt(np.maximum(0, obsnorm_meansq - np.square(obsnorm_mean)))\n",
    "        print('obs', obsnorm_mean.shape, obsnorm_stdev.shape)\n",
    "        normedobs_bo = (obs_bo - obsnorm_mean) / (obsnorm_stdev + 1e-6) # 1e-6 constant from Standardizer class in nn.py:409 in openai/imitation\n",
    "\n",
    "        curr_activations_bd = normedobs_bo\n",
    "\n",
    "        # Hidden layers next\n",
    "        assert list(policy_params['hidden'].keys()) == ['FeedforwardNet']\n",
    "        layer_params = policy_params['hidden']['FeedforwardNet']\n",
    "        for layer_name in sorted(layer_params.keys()):\n",
    "            l = layer_params[layer_name]\n",
    "            W, b = read_layer(l)\n",
    "            curr_activations_bd = apply_nonlin(tf.matmul(curr_activations_bd, W) + b)\n",
    "\n",
    "        # Output layer\n",
    "        W, b = read_layer(policy_params['out'])\n",
    "        output_bo = tf.matmul(curr_activations_bd, W) + b\n",
    "        return output_bo\n",
    "\n",
    "    obs_bo = tf.placeholder(tf.float32, [None, None])\n",
    "    a_ba = build_policy(obs_bo)\n",
    "    policy_fn = tf_util.function([obs_bo], a_ba)\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n",
      "iter 0\n",
      "(111,)\n",
      "(1, 111)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-25c0091c90a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mobservations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "#import load_policy\n",
    "import tf_util\n",
    "print('loading and building expert policy')\n",
    "policy_fn = load_policy('experts/Ant-v2.pkl')\n",
    "print('loaded and built')\n",
    "\n",
    "# test the policy\n",
    "with tf.Session():\n",
    "    tf_util.initialize()\n",
    "\n",
    "    env = gym.make('Ant-v2')\n",
    "    #max_steps = args.max_timesteps or env.spec.timestep_limit\n",
    "    max_steps = env._max_episode_steps\n",
    "\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    for i in range(10):\n",
    "        print('iter', i)\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0.\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            print(obs.shape)\n",
    "            print(obs[None,:].shape)\n",
    "            print(model)\n",
    "            assert False\n",
    "            action = policy_fn(obs[None,:])\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "\n",
    "            #if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)\n",
    "        print(totalr)\n",
    "\n",
    "    print('returns', returns)\n",
    "    print('mean return', np.mean(returns))\n",
    "    print('std of return', np.std(returns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle, tensorflow as tf, tf_util, numpy as np\n",
    "\n",
    "def load_policy(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.loads(f.read())\n",
    "\n",
    "    # assert len(data.keys()) == 2\n",
    "    nonlin_type = data['nonlin_type']\n",
    "    policy_type = [k for k in data.keys() if k != 'nonlin_type'][0]\n",
    "\n",
    "    assert policy_type == 'GaussianPolicy', 'Policy type {} not supported'.format(policy_type)\n",
    "    policy_params = data[policy_type]\n",
    "\n",
    "    assert set(policy_params.keys()) == {'logstdevs_1_Da', 'hidden', 'obsnorm', 'out'}\n",
    "\n",
    "    # Keep track of input and output dims (i.e. observation and action dims) for the user\n",
    "\n",
    "    def build_policy(obs_bo):\n",
    "        def read_layer(l):\n",
    "            assert list(l.keys()) == ['AffineLayer']\n",
    "            assert sorted(l['AffineLayer'].keys()) == ['W', 'b']\n",
    "            return l['AffineLayer']['W'].astype(np.float32), l['AffineLayer']['b'].astype(np.float32)\n",
    "\n",
    "        def apply_nonlin(x):\n",
    "            if nonlin_type == 'lrelu':\n",
    "                return tf_util.lrelu(x, leak=.01) # openai/imitation nn.py:233\n",
    "            elif nonlin_type == 'tanh':\n",
    "                return tf.tanh(x)\n",
    "            else:\n",
    "                raise NotImplementedError(nonlin_type)\n",
    "\n",
    "        # Build the policy. First, observation normalization.\n",
    "        assert list(policy_params['obsnorm'].keys()) == ['Standardizer']\n",
    "        obsnorm_mean = policy_params['obsnorm']['Standardizer']['mean_1_D']\n",
    "        obsnorm_meansq = policy_params['obsnorm']['Standardizer']['meansq_1_D']\n",
    "        obsnorm_stdev = np.sqrt(np.maximum(0, obsnorm_meansq - np.square(obsnorm_mean)))\n",
    "        print('obs', obsnorm_mean.shape, obsnorm_stdev.shape)\n",
    "        normedobs_bo = (obs_bo - obsnorm_mean) / (obsnorm_stdev + 1e-6) # 1e-6 constant from Standardizer class in nn.py:409 in openai/imitation\n",
    "\n",
    "        curr_activations_bd = normedobs_bo\n",
    "\n",
    "        # Hidden layers next\n",
    "        assert list(policy_params['hidden'].keys()) == ['FeedforwardNet']\n",
    "        layer_params = policy_params['hidden']['FeedforwardNet']\n",
    "        for layer_name in sorted(layer_params.keys()):\n",
    "            l = layer_params[layer_name]\n",
    "            W, b = read_layer(l)\n",
    "            curr_activations_bd = apply_nonlin(tf.matmul(curr_activations_bd, W) + b)\n",
    "\n",
    "        # Output layer\n",
    "        W, b = read_layer(policy_params['out'])\n",
    "        output_bo = tf.matmul(curr_activations_bd, W) + b\n",
    "        return output_bo\n",
    "\n",
    "    obs_bo = tf.placeholder(tf.float32, [None, None])\n",
    "    a_ba = build_policy(obs_bo)\n",
    "    policy_fn = tf_util.function([obs_bo], a_ba)\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "\n",
    "def agent_wapper(config, agent):\n",
    "    def fn(obs):\n",
    "        with torch.no_grad():\n",
    "            obs = obs.astype(np.float32)\n",
    "            assert len(obs.shape) == 2\n",
    "            obs = torch.from_numpy(obs).to(config.device)\n",
    "            action = agent(obs)\n",
    "        return action.cpu().numpy()\n",
    "    return fn\n",
    "\n",
    "def fit_dataset(config, agent, dataset, n_epochs):\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=config.lr, weight_decay=config.L2)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    step = 0\n",
    "    best_reward = None\n",
    "    loss_his = []\n",
    "    \n",
    "    for k in range(n_epochs):\n",
    "        for batch in dataloader:\n",
    "            obs, gold_actions = batch\n",
    "            pred_actions = agent(obs)\n",
    "            loss = loss_fn(pred_actions, gold_actions)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_his.append(loss.item())\n",
    "\n",
    "            if step % config.eval_steps == 0:\n",
    "                avrg_mean, avrg_std = Eval(config, agent_wapper(config, agent))\n",
    "                avrg_loss = np.mean(loss_his)\n",
    "                loss_his = []\n",
    "                print('[epoch {}  step {}] loss: {:.4f}  r_mean: {:.2f}  r_std: {:.2f}'.format(\n",
    "                    k + 1, step, avrg_loss, avrg_mean, avrg_std))\n",
    "\n",
    "                avrg_reward = avrg_mean - avrg_std\n",
    "                if best_reward is None or best_reward < avrg_reward:\n",
    "                    best_reward = avrg_reward\n",
    "                    save_model(config, agent, config.model_save_path)\n",
    "                \n",
    "            step += 1\n",
    "    \n",
    "    load_model(config, agent, config.model_save_path)\n",
    "\n",
    "def BehavioralCloning(config, agent, expert):\n",
    "\n",
    "    # get expert demonstration\n",
    "    expert_obs, expert_actions, *_ = run_agent(config, expert, config.n_expert_rollouts)\n",
    "    expert_obs = torch.from_numpy(expert_obs).to(config.device)\n",
    "    expert_actions = torch.from_numpy(expert_actions).to(config.device)\n",
    "    dataset = TensorDataset(expert_obs, expert_actions)\n",
    "\n",
    "    # training agent\n",
    "    fit_dataset(config, agent, dataset, config.epochs)\n",
    "\n",
    "    return agent_wapper(config, agent)\n",
    "\n",
    "def DAgger(config, agent, expert):\n",
    "    # get expert demonstration\n",
    "    expert_obs, expert_actions, *_ = run_agent(config, expert, config.n_expert_rollouts)\n",
    "    expert_obs = torch.from_numpy(expert_obs).to(config.device)\n",
    "    expert_actions = torch.from_numpy(expert_actions).to(config.device)\n",
    "    dataset = TensorDataset(expert_obs, expert_actions)\n",
    "\n",
    "    for k in range(config.n_dagger_iter):\n",
    "        # training agent\n",
    "        fit_dataset(config, agent, dataset, config.epochs)\n",
    "        \n",
    "        # run agent to get new on-policy observations\n",
    "        new_obs, *_ = run_agent(config, agent_wapper(config, agent), config.n_dagger_rollouts)\n",
    "        expert_actions = expert(new_obs)\n",
    "        \n",
    "        new_obs = torch.from_numpy(new_obs).to(config.device)\n",
    "        expert_actions = torch.from_numpy(expert_actions).to(config.device)\n",
    "        new_data = TensorDataset(new_obs, expert_actions)\n",
    "        \n",
    "        # add new data to dataset\n",
    "        dataset = ConcatDataset([dataset, new_data])\n",
    "            \n",
    "\n",
    "        avrg_mean, avrg_std = Eval(config, agent_wapper(config, agent))\n",
    "        print('[DAgger iter {}] r_mean: {:.2f}  r_std: {:.2f}'.format(k + 1, avrg_mean, avrg_std))\n",
    "\n",
    "        \n",
    "    return agent_wapper(config, agent)\n",
    "    \n",
    "def run_agent(config, agent, num_rollouts):\n",
    "    env = config.env\n",
    "    max_steps = env.spec.timestep_limit\n",
    "    o_dim = env.observation_space.shape[0]\n",
    "\n",
    "    returns = []\n",
    "    observations = []\n",
    "    actions = []\n",
    "    for _ in range(num_rollouts):\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        totalr = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            print(obs.reshape(-1,o_dim).shape)\n",
    "            print(agent)\n",
    "            assert False\n",
    "            action = agent(obs.reshape(-1,o_dim))\n",
    "            action = action.reshape(-1)\n",
    "            observations.append(obs)\n",
    "            actions.append(action)\n",
    "            obs, r, done, _ = env.step(action)\n",
    "            totalr += r\n",
    "            steps += 1\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "        returns.append(totalr)\n",
    "\n",
    "    avrg_mean, avrg_std = np.mean(returns), np.std(returns)\n",
    "    observations = np.array(observations).astype(np.float32)\n",
    "    actions = np.array(actions).astype(np.float32)\n",
    "\n",
    "    return observations, actions, avrg_mean, avrg_std\n",
    "\n",
    "def Eval(config, agent):\n",
    "    *_, avrg_mean, avrg_std = run_agent(config, agent, config.n_eval_rollouts)\n",
    "\n",
    "    return avrg_mean, avrg_std\n",
    "\n",
    "\n",
    "def save_model(config, model, PATH):\n",
    "    if not os.path.exists(PATH):\n",
    "        os.makedirs(PATH)\n",
    "    PATH = PATH + config.envname + '-' + 'parameters.tar'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "    print('model saved.')\n",
    "\n",
    "def load_model(config, model, PATH):\n",
    "    PATH = PATH + config.envname + '-' + 'parameters.tar'\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    print('model loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Humanoid-v2 BC ********************\n",
      "obs (1, 376) (1, 376)\n",
      "(1, 376)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-696e18955280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-696e18955280>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'BC'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBehavioralCloning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'DA'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAgger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-4aa8229ea5a0>\u001b[0m in \u001b[0;36mBehavioralCloning\u001b[0;34m(config, agent, expert)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# get expert demonstration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mexpert_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert_actions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_expert_rollouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0mexpert_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_obs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mexpert_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpert_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-58-4aa8229ea5a0>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(config, agent, num_rollouts)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "#from load_policy import load_policy\n",
    "\n",
    "class Config():\n",
    "    seed = 3\n",
    "    envname = 'Humanoid-v2'\n",
    "    env = gym.make(envname)\n",
    "    method = 'BC' # BC: Behavioral Cloning   DA: DAgger\n",
    "    device = torch.device('cpu')\n",
    "    expert_path = './experts/'\n",
    "    model_save_path = './models/'\n",
    "    n_expert_rollouts = 30 # number of rollouts from expert\n",
    "    n_dagger_rollouts = 10 # number of new rollouts from learned model for a DAgger iteration\n",
    "    n_dagger_iter = 10 # number of DAgger iterations\n",
    "    n_eval_rollouts = 10 # number of rollouts for evaluating a policy\n",
    "    L2 = 0.00001\n",
    "    lr = 0.0001\n",
    "    epochs = 20\n",
    "    batch_size = 64\n",
    "\n",
    "    eval_steps = 500\n",
    "\n",
    "    \n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Agent, self).__init__()\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs):\n",
    "        return self.mlp(obs)\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = Config()\n",
    "    print('*' * 20, config.envname, config.method, '*' * 20)\n",
    "    env = config.env\n",
    "    if config.seed:\n",
    "        env.seed(config.seed)\n",
    "        torch.manual_seed(config.seed)\n",
    "    agent = Agent(env.observation_space.shape[0], env.action_space.shape[0]).to(config.device)\n",
    "    expert = load_policy(config.expert_path + config.envname + '.pkl')\n",
    "    method = config.method\n",
    "\n",
    "    if method == 'BC':\n",
    "        agent = BehavioralCloning(config, agent, expert)\n",
    "    elif method == 'DA':\n",
    "        agent = DAgger(config, agent, expert)\n",
    "    else:\n",
    "        NotImplementedError(method)\n",
    "\n",
    "    \n",
    "    avrg_mean, avrg_std = Eval(config, expert)\n",
    "    print('[expert] avrg_mean:{:.2f}  avrg_std:{:.2f}'.format(avrg_mean, avrg_std))\n",
    "        \n",
    "    avrg_mean, avrg_std = Eval(config, agent)\n",
    "    print('[agent] avrg_mean:{:.2f}  avrg_std:{:.2f}'.format(avrg_mean, avrg_std))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
